{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "from pathlib import Path\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "# import spacy\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avx2\n"
     ]
    }
   ],
   "source": [
    "print(tp.isa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add redundant words to stop words\n",
    "en_stop.add(\"said\")\n",
    "en_stop.add(\"reuters\")\n",
    "en_stop.add(\"london\")\n",
    "en_stop.add(\"new york\")\n",
    "en_stop.add('reuters')\n",
    "en_stop.add('say')\n",
    "en_stop.add('like')\n",
    "en_stop.add('thing')\n",
    "en_stop.add('york')\n",
    "en_stop.add('new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"../results/models/tomotopy\")\n",
    "MODEL0 = \"test-hlda0.tmm\"\n",
    "MODEL1 = \"test-hlda1.tmm\"\n",
    "MODEL2 = \"test-hlda2.tmm\"\n",
    "MODEL3 = \"test-hlda3.tmm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from file\n",
    "mdl0 = tp.HLDAModel.load(f\"{MODEL_PATH}/{MODEL0}\")\n",
    "mdl1 = tp.HLDAModel.load(f\"{MODEL_PATH}/{MODEL1}\")\n",
    "mdl2 = tp.HLDAModel.load(f\"{MODEL_PATH}/{MODEL2}\")\n",
    "mdl3 = tp.HLDAModel.load(f\"{MODEL_PATH}/{MODEL3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = {#name the clusters as seems reasonable\n",
    "    0: \"Science and Technology\",#tech and business\n",
    "    1: \"Health/Science/Drugs\",#health\n",
    "    2: \"Business/Tech/BioTech\",#business\n",
    "    3: \"Entertainment/News\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(unseen_doc):\n",
    "    \"\"\"\n",
    "    Preprocesses text and returns tomotopy corups.\n",
    "    \n",
    "    unseen_doc: str\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    pat = re.compile('\\w+')\n",
    "\n",
    "    corpus = tp.utils.Corpus(\n",
    "            tokenizer = tp.utils.SimpleTokenizer(stemmer=None, lowercase=True), \n",
    "            stopwords = lambda x: len(x) <= 2 or x in en_stop or x.isnumeric() or not pat.match(x) or not lemmatizer.lemmatize(x)\n",
    "        )\n",
    "\n",
    "    corpus.process(document.lower() for document in unseen_doc)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def get_best_model(list_of_models, document_text):\n",
    "    \"\"\"\n",
    "    Takes list of HLDAModels and string of article text.\n",
    "    Returns best HLDAModel and topic distribution of \n",
    "    document_text.\n",
    "    \"\"\"\n",
    "\n",
    "    corpus = process_text(document_text)\n",
    "\n",
    "    mdl_results=[]\n",
    "    for mdl in list_of_models:\n",
    "        topic_dist, ll = mdl.infer(corpus)\n",
    "        mdl_results.append((topic_dist, ll))\n",
    "\n",
    "    max_ll = max(mdl_results, key=lambda item: item[1])[1]\n",
    "    max_topic_dist = max(mdl_results,key=lambda item: item[1])[0]\n",
    "    max_index = mdl_results.index((max_topic_dist, max_ll))\n",
    "    mdl_final = mdls[max_index]\n",
    "\n",
    "    return mdl_final, max_topic_dist[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bitcoin continued to slide after a broader stock sell-off in the U.S. last week sent the cryptocurrency market into a frenzy and prompted bitcoin to plummet by roughly 10%. Bitcoin, the world’s largest digital currency by market value, was lower by about 3% at $33,438.03 late Sunday, according to data from Coin Metrics. This year, Bitcoin has been trading in a narrow range as it attempts to reclaim its highs of late 2021. The cryptocurrency is now down 50% from its peak price of $67,802.30 in November 2021. The drop comes after the blue-chip Dow Jones Industrial Average lost more than 1,000 points on Thursday and the Nasdaq plunged by 5%. Those losses marked the worst single-day drops since 2020. The Dow and Nasdaq fell again on Friday.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample text\n",
    "other_texts = {'txt':['Bitcoin continued to slide after a broader stock sell-off in the U.S. last week sent the cryptocurrency market into a frenzy and prompted bitcoin to plummet by roughly 10%. Bitcoin, the world’s largest digital currency by market value, was lower by about 3% at $33,438.03 late Sunday, according to data from Coin Metrics. This year, Bitcoin has been trading in a narrow range as it attempts to reclaim its highs of late 2021. The cryptocurrency is now down 50% from its peak price of $67,802.30 in November 2021. The drop comes after the blue-chip Dow Jones Industrial Average lost more than 1,000 points on Thursday and the Nasdaq plunged by 5%. Those losses marked the worst single-day drops since 2020. The Dow and Nasdaq fell again on Friday.']}\n",
    "unseen = other_texts['txt']\n",
    "unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tomotopy.HLDAModel at 0x16a58e9ec30>, array([  0,  13, 161]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdls = [mdl0, mdl1 ,mdl2, mdl3]\n",
    "final_model, topic_dist = get_best_model(mdls, unseen)\n",
    "final_model, topic_dist.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model depth: 3\n",
      "number of topic: 264\n"
     ]
    }
   ],
   "source": [
    "print(f\"model depth: {final_model.depth}\")\n",
    "print(f\"number of topic: {final_model.k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "feb9fbdaf8823060a978ec464cb9b002124d7b95a0c8d909409fa113c0eca975"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fourthbrain-glg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
