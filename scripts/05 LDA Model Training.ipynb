{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, ldamulticore\n",
    "from gensim.models.callbacks import PerplexityMetric, ConvergenceMetric, CoherenceMetric\n",
    "from itertools import product\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"sample50k_health_tech\"\n",
    "\n",
    "DICTIONARY_PATH = Path(\"../results/dictionary\")\n",
    "CORPUS_PATH = Path(\"../results/corpus\")\n",
    "MODEL_PATH = Path(\"../results/models\")/DATASET\n",
    "\n",
    "MODEL_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log in to W&B account\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved dictionary and corpus\n",
    "dictionary = corpora.Dictionary.load(f\"{DICTIONARY_PATH/DATASET}\")\n",
    "corpus = corpora.MmCorpus(f\"{CORPUS_PATH/DATASET}.mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 70567\n",
      "Number of documents: 29360\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save experiments to Weighths & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single core version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the callbacks loggers\n",
    "perplexity_logger = PerplexityMetric(corpus=corpus, logger='shell')\n",
    "convergence_logger = ConvergenceMetric(logger='shell', normed=True)\n",
    "coherence_umass_logger = CoherenceMetric(corpus=corpus, logger='shell', coherence = 'u_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_passes = [4]\n",
    "num_iterations = [20, 50] # default is 50\n",
    "num_topics = [8, 12, 16, 24, 32]\n",
    "all_combinations = list(product(num_passes, num_iterations, num_topics))\n",
    "\n",
    "for passes, iterations, topics in all_combinations:\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    config = {\n",
    "        \"topics\": topics,\n",
    "        \"passes\": passes,\n",
    "        \"iterations\": iterations,\n",
    "        \"coherence_metric\": 'u_mass',\n",
    "        \"model\": \"gensim.models.LdaModel\",\n",
    "        \"random_state\": 100\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"lda-labelled-subset\",\n",
    "        entity=\"angelika\",\n",
    "        name = \"i{}p{}t{}\".format(iterations, passes, topics),\n",
    "#         notes=\"finding number of passes and iterations\",\n",
    "        tags=[DATASET],\n",
    "        config=config,\n",
    "    )\n",
    "   \n",
    "    # Create model - note callbacks argument uses list of created callback loggers\n",
    "    model = LdaModel(corpus=corpus,\n",
    "             id2word=dictionary,\n",
    "             num_topics=topics,\n",
    "            #  eval_every=20,\n",
    "             passes=passes,\n",
    "             iterations=iterations,\n",
    "             random_state=100,         \n",
    "            callbacks=[convergence_logger, perplexity_logger, coherence_umass_logger])\n",
    "\n",
    "    # Log metrics\n",
    "    for con, coh, per in zip(model.metrics[\"Convergence\"], model.metrics[\"Coherence\"], model.metrics[\"Perplexity\"]):\n",
    "        wandb.log({\"Convergence\": con,\n",
    "                  \"Coherence\": coh,\n",
    "                  \"Perplexity\": per})\n",
    "        \n",
    "    time_elapsed = datetime.now() - start_time   \n",
    "    wandb.log({\"time_elapsed\": str(time_elapsed)})\n",
    "    \n",
    "    # save model\n",
    "    model_fn = f\"{MODEL_PATH}/lda_{iterations}i{passes}p{topics}t.model\"\n",
    "    model.save(model_fn)\n",
    "    wandb.log({\"model_name\": model_fn})\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicore version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_passes = [5] # default 1\n",
    "num_iterations = [10, 20, 50] # default 50\n",
    "num_topics = [4, 8, 12, 24, 48]\n",
    "\n",
    "# Create experiments with different number of passes and iterations\n",
    "run_combinations = list(product(num_passes, num_iterations))\n",
    "\n",
    "for passes, iterations in run_combinations:\n",
    "\n",
    "    # Start new W&B run\n",
    "    config = {\n",
    "        \"topics\": topics,\n",
    "        \"passes\": passes,\n",
    "        \"iteration\": iteration,\n",
    "        \"coherence_metric\": 'u_mass',\n",
    "        \"model\": \"models.ldamulticore.LdaMulticore\",\n",
    "        \"workers\": 5 # number of cores minus 1\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"lda-labelled-subset\",\n",
    "        entity=\"angelika\",\n",
    "        name = \"i{}p{}t{}\".format(iteration, passes, str(topics)),\n",
    "        tags=[\"sample50k_topic4\"],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "\n",
    "    for topic in topics:\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Multicore model does not support callbacks\n",
    "        model = ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                id2word=dictionary,\n",
    "                num_topics=topic,\n",
    "                passes=passes,\n",
    "                iterations=iteration,\n",
    "                random_state=100,\n",
    "                workers=5)\n",
    "\n",
    "        # Calculate metrics\n",
    "        cm = CoherenceModel(model=model, corpus=corpus, coherence='u_mass', topn=20)\n",
    "        coherence = cm.get_coherence()\n",
    "\n",
    "        cm2 = Co\n",
    "        \n",
    "        time_elapsed = datetime.now() - start_time\n",
    "        \n",
    "        wandb.log({\"Coherence\": coherence,\n",
    "                \"time_elapsed\": round(time_elapsed.total_seconds()/60, 2),\n",
    "                \"num_topic\": topic\n",
    "                })\n",
    "\n",
    "        # save model\n",
    "        model_fn = f\"{MODEL_PATH}/lda_{iterations}i{passes}p{topics}t.model\"\n",
    "        model.save(model_fn)    \n",
    "        wandb.log({\"model_name\": model_fn})\n",
    "        \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "feb9fbdaf8823060a978ec464cb9b002124d7b95a0c8d909409fa113c0eca975"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fourthbrain-glg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
